{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 0.00 MB\n",
      "Peak GPU memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import cuda\n",
    "torch.cuda.empty_cache()\n",
    "current_mem = torch.cuda.memory_allocated() / (1024 * 1024)  # Convert bytes to megabytes\n",
    "\n",
    "# Peak memory usage on GPU\n",
    "peak_mem = torch.cuda.max_memory_allocated() / (1024 * 1024)  # Convert bytes to megabytes\n",
    "\n",
    "print(f\"Current GPU memory usage: {current_mem:.2f} MB\")\n",
    "print(f\"Peak GPU memory usage: {peak_mem:.2f} MB\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T21:41:53.398920100Z",
     "start_time": "2023-12-09T21:41:51.508629Z"
    }
   },
   "id": "871907f347c48ab5"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-09T21:41:54.129814400Z",
     "start_time": "2023-12-09T21:41:53.399911700Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['sentence', 'label'],\n        num_rows: 2264\n    })\n})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"financial_phrasebank\", 'sentences_allagree')\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T21:41:55.024249Z",
     "start_time": "2023-12-09T21:41:54.238039300Z"
    }
   },
   "id": "29b6e1c23e34c7d0"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T21:41:55.054056500Z",
     "start_time": "2023-12-09T21:41:55.044186600Z"
    }
   },
   "id": "d1c51618e63a73b9"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#nltk.download('all')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T21:41:55.091749900Z",
     "start_time": "2023-12-09T21:41:55.057575300Z"
    }
   },
   "id": "26abd8601eb24b73"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "sentences = dataset['train']['sentence']\n",
    "labels = dataset['train']['label']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T21:41:55.149698900Z",
     "start_time": "2023-12-09T21:41:55.149698900Z"
    }
   },
   "id": "39d6233526c4169b"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Sentences    object\nLabels        int64\ndtype: object"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.DataFrame()\n",
    "train['Sentences'] = sentences\n",
    "train['Labels'] = labels\n",
    "train['Sentences'] = train['Sentences'].astype(str)\n",
    "train.dtypes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T21:41:55.150708100Z",
     "start_time": "2023-12-09T21:41:55.149698900Z"
    }
   },
   "id": "58a3351eed12c8ce"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from preprocess import preprocess_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T21:41:55.151697500Z",
     "start_time": "2023-12-09T21:41:55.150708100Z"
    }
   },
   "id": "4cb2544e17e7805c"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T21:41:55.152715900Z",
     "start_time": "2023-12-09T21:41:55.150708100Z"
    }
   },
   "id": "80d89cbe774eec87"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "                                              Sentences  Labels\n0     according gran company plan move production ru...       1\n1     last quarter componenta net sale doubled eur e...       2\n2     third quarter net sale increased eur mn operat...       2\n3     operating profit rose eur mn eur mn correspond...       2\n4     operating profit totalled eur mn eur mn repres...       2\n...                                                 ...     ...\n2259  operating result month period decreased profit...       0\n2260  helsinki thomson financial share cargotec fell...       0\n2261  london marketwatch share price ended lower lon...       0\n2262  operating profit fell eur mn eur mn including ...       0\n2263  sale finland decreased january sale outside fi...       0\n\n[2264 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentences</th>\n      <th>Labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>according gran company plan move production ru...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>last quarter componenta net sale doubled eur e...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>third quarter net sale increased eur mn operat...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>operating profit rose eur mn eur mn correspond...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>operating profit totalled eur mn eur mn repres...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2259</th>\n      <td>operating result month period decreased profit...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2260</th>\n      <td>helsinki thomson financial share cargotec fell...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2261</th>\n      <td>london marketwatch share price ended lower lon...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2262</th>\n      <td>operating profit fell eur mn eur mn including ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2263</th>\n      <td>sale finland decreased january sale outside fi...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2264 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Sentences'] = train['Sentences'].apply(preprocess_df)\n",
    "train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T21:41:57.650089600Z",
     "start_time": "2023-12-09T21:41:55.150708100Z"
    }
   },
   "id": "6bd119f26c8cfc94"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(train, test_size=0.3, random_state=42)\n",
    "train_labels = torch.tensor(np.array(train_df['Labels']))\n",
    "test_labels = torch.tensor(np.array(test_df['Labels']))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T21:41:57.655113600Z",
     "start_time": "2023-12-09T21:41:57.648093300Z"
    }
   },
   "id": "3b97f305e2fe29c8"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T21:41:58.916363400Z",
     "start_time": "2023-12-09T21:41:57.655113600Z"
    }
   },
   "id": "7cb5cfde0d5be737"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "max_length = 512  # Set the maximum sequence length\n",
    "batch_size = 16\n",
    "num_epochs = 4"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T21:41:59.001858600Z",
     "start_time": "2023-12-09T21:41:58.923344500Z"
    }
   },
   "id": "bf189d0bb1f2f270"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T21:41:59.025796400Z",
     "start_time": "2023-12-09T21:41:58.941299200Z"
    }
   },
   "id": "d0f81a122298f60f"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def tokenize_data(data):\n",
    "    tokenized = tokenizer.batch_encode_plus(\n",
    "        data,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return tokenized\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T21:41:59.027783Z",
     "start_time": "2023-12-09T21:41:59.010830600Z"
    }
   },
   "id": "b0a400818a608e1"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "train_tokenized = tokenize_data(train_df['Sentences'].tolist())\n",
    "test_tokenized = tokenize_data(test_df['Sentences'].tolist())\n",
    "\n",
    "train_dataset = TensorDataset(train_tokenized['input_ids'], train_tokenized['attention_mask'], train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_tokenized['input_ids'], test_tokenized['attention_mask'], test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T21:42:01.053130600Z",
     "start_time": "2023-12-09T21:41:59.010830600Z"
    }
   },
   "id": "f07c8ab1fc259027"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input_ids size: torch.Size([1584, 512])\n",
      "Train attention_mask size: torch.Size([1584, 512])\n",
      "Train labels size: torch.Size([1584])\n",
      "Test input_ids size: torch.Size([680, 512])\n",
      "Test attention_mask size: torch.Size([680, 512])\n",
      "Test labels size: torch.Size([680])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train input_ids size:\", train_tokenized['input_ids'].size())\n",
    "print(\"Train attention_mask size:\", train_tokenized['attention_mask'].size())\n",
    "print(\"Train labels size:\", train_labels.size())\n",
    "\n",
    "print(\"Test input_ids size:\", test_tokenized['input_ids'].size())\n",
    "print(\"Test attention_mask size:\", test_tokenized['attention_mask'].size())\n",
    "print(\"Test labels size:\", test_labels.size())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T21:42:01.069084900Z",
     "start_time": "2023-12-09T21:42:01.053130600Z"
    }
   },
   "id": "450bd528bfb336a"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training: 100%|██████████| 99/99 [00:44<00:00,  2.24batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6338 - Train Accuracy: 72.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Testing: 100%|██████████| 43/43 [00:06<00:00,  6.97batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3511 - Test Accuracy: 87.06%\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training: 100%|██████████| 99/99 [00:43<00:00,  2.28batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2567 - Train Accuracy: 90.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Testing: 100%|██████████| 43/43 [00:06<00:00,  6.93batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2295 - Test Accuracy: 91.32%\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training: 100%|██████████| 99/99 [00:43<00:00,  2.27batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1423 - Train Accuracy: 94.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Testing: 100%|██████████| 43/43 [00:06<00:00,  6.90batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2810 - Test Accuracy: 90.44%\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training: 100%|██████████| 99/99 [00:43<00:00,  2.26batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1158 - Train Accuracy: 95.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Testing: 100%|██████████| 43/43 [00:06<00:00,  6.85batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2711 - Test Accuracy: 90.44%\n",
      "Training finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    correct_train_predictions = 0\n",
    "    total_train_predictions = 0\n",
    "\n",
    "    for step, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1} - Training\", unit=\"batch\")):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels.unsqueeze(1))\n",
    "        loss = outputs.loss\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        total_train_predictions += labels.size(0)\n",
    "        correct_train_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate train accuracy and loss for the epoch\n",
    "    train_epoch_loss = running_train_loss / len(train_loader)\n",
    "    train_epoch_accuracy = (correct_train_predictions / total_train_predictions) * 100\n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f} - Train Accuracy: {train_epoch_accuracy:.2f}%\")\n",
    "\n",
    "    # Validation (testing) after each epoch\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    correct_test_predictions = 0\n",
    "    total_test_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_step, test_batch in enumerate(tqdm(test_loader, desc=f\"Epoch {epoch + 1} - Testing\", unit=\"batch\")):\n",
    "            test_input_ids, test_attention_mask, test_labels = test_batch\n",
    "            test_input_ids = test_input_ids.to(device)\n",
    "            test_attention_mask = test_attention_mask.to(device)\n",
    "            test_labels = test_labels.to(device)\n",
    "\n",
    "            test_outputs = model(test_input_ids, attention_mask=test_attention_mask, labels=test_labels.unsqueeze(1))\n",
    "            test_loss = test_outputs.loss\n",
    "            running_test_loss += test_loss.item()\n",
    "\n",
    "            _, test_predicted = torch.max(test_outputs.logits, 1)\n",
    "            total_test_predictions += test_labels.size(0)\n",
    "            correct_test_predictions += (test_predicted == test_labels).sum().item()\n",
    "\n",
    "    # Calculate test accuracy and loss for the epoch\n",
    "    test_epoch_loss = running_test_loss / len(test_loader)\n",
    "    test_epoch_accuracy = (correct_test_predictions / total_test_predictions) * 100\n",
    "    print(f\"Test Loss: {test_epoch_loss:.4f} - Test Accuracy: {test_epoch_accuracy:.2f}%\")\n",
    "\n",
    "print(\"Training finished.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T21:45:21.092687800Z",
     "start_time": "2023-12-09T21:42:01.065096300Z"
    }
   },
   "id": "2684e0231518c11"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "       Sentences                                                            \\\n           count unique                                                top   \nLabels                                                                       \n0            303    297                operating profit fell eur mn eur mn   \n1           1391   1370                                 value order eur mn   \n2            570    546  operating profit rose eur mn eur mn correspond...   \n\n             \n       freq  \nLabels       \n0         3  \n1         7  \n2         6  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"4\" halign=\"left\">Sentences</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>count</th>\n      <th>unique</th>\n      <th>top</th>\n      <th>freq</th>\n    </tr>\n    <tr>\n      <th>Labels</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>303</td>\n      <td>297</td>\n      <td>operating profit fell eur mn eur mn</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1391</td>\n      <td>1370</td>\n      <td>value order eur mn</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>570</td>\n      <td>546</td>\n      <td>operating profit rose eur mn eur mn correspond...</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby('Labels').describe()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T21:52:23.295373400Z",
     "start_time": "2023-12-09T21:52:23.251546900Z"
    }
   },
   "id": "47da3d72057b6625"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As can be seen, theres a class imblance between the labels, and even though the base bert model is performing pretty good on the test set with an accuracy of 90.44% on the final epoch, it is still overfitting on the train data with the accuracy being 95.83% on the final epoch. One possible solution to this is to introduce augmentations in the data to reduce the class imbalance and boost the model performance on the test set.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58cdc4ced767e381"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6192d37580ae9af1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
